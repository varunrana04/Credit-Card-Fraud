{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310,"isSourceIdPinned":false}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# Credit Card Fraud Detection - Ensemble + Full SHAP Explainability (incl. interactions)\n# ============================================================\nimport os, json, joblib, warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, confusion_matrix,\n    roc_curve, precision_recall_curve, average_precision_score,\n)\n\nimport lightgbm as lgb\nimport shap\n\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams[\"figure.dpi\"] = 150\n\n# ============================================================\n# Paths\n# ============================================================\nBASE = Path(\"/kaggle/working\")\nDATA = Path(\"/kaggle/input/creditcardfraud/creditcard.csv\")\nOUT = BASE / \"out/fraud_model\"\nPROC = BASE / \"data/fraud/processed\"\nOUT.mkdir(parents=True, exist_ok=True)\nPROC.mkdir(parents=True, exist_ok=True)\n\n# ============================================================\n# Load Data\n# ============================================================\ndf = pd.read_csv(DATA)\nprint(\"Data shape:\", df.shape)\nprint(\"Columns:\", df.columns.tolist())\n\n# Features and target\nX = df.drop(\"Class\", axis=1).copy()\ny = df[\"Class\"].copy()\n\n# Train-test split (stratified)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Scale Amount; drop Time\nscaler_amount = StandardScaler()\nX_train = X_train.copy()\nX_test = X_test.copy()\nX_train.loc[:, \"normAmount\"] = scaler_amount.fit_transform(X_train[[\"Amount\"]])\nX_test.loc[:,  \"normAmount\"] = scaler_amount.transform(X_test[[\"Amount\"]])\n\nX_train.drop([\"Time\", \"Amount\"], axis=1, inplace=True)\nX_test.drop([\"Time\", \"Amount\"], axis=1, inplace=True)\n\n# Save feature order & scaler\nfeature_order = list(X_train.columns)\njson.dump(feature_order, open(PROC / \"feature_order.json\", \"w\"))\njoblib.dump(scaler_amount, PROC / \"scaler_normAmount.pkl\")\nprint(\"Processed artifacts saved to:\", PROC)\nprint(os.listdir(PROC))\n\n# ============================================================\n# Train Ensemble Models\n# ============================================================\ndef train_seed(seed):\n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dval = lgb.Dataset(X_test, label=y_test)\n\n    pos_count = int(y_train.sum())\n    neg_count = len(y_train) - pos_count\n    spw = neg_count / max(1, pos_count)\n\n    params = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"learning_rate\": 0.01,\n        \"num_leaves\": 31,\n        \"min_data_in_leaf\": 50,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 5,\n        \"verbosity\": -1,\n        \"seed\": seed,\n        \"scale_pos_weight\": float(min(50.0, spw)),  # clip for stability\n    }\n    model = lgb.train(\n        params,\n        dtrain,\n        valid_sets=[dval],\n        num_boost_round=2000,\n        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)],\n    )\n    return model\n\nseeds = [11, 22, 33]\nmodels = [train_seed(s) for s in seeds]\n\n# Save models\nfor i, m in enumerate(models):\n    joblib.dump(m, OUT / f\"lgb_fraud_ens_seed_{seeds[i]}.pkl\")\n\n# ============================================================\n# Ensemble Predictions & Metrics\n# ============================================================\nprobs = np.mean([m.predict(X_test, num_iteration=m.best_iteration) for m in models], axis=0)\ny_pred_05 = (probs >= 0.5).astype(int)\n\nmetrics = {\n    \"accuracy@0.5\": float(accuracy_score(y_test, y_pred_05)),\n    \"precision@0.5\": float(precision_score(y_test, y_pred_05, zero_division=0)),\n    \"recall@0.5\": float(recall_score(y_test, y_pred_05, zero_division=0)),\n    \"f1@0.5\": float(f1_score(y_test, y_pred_05, zero_division=0)),\n    \"roc_auc\": float(roc_auc_score(y_test, probs)),\n    \"avg_precision\": float(average_precision_score(y_test, probs)),\n}\nprint(\"Metrics:\", json.dumps(metrics, indent=2))\n\n# Best F1 threshold search\nths = np.linspace(0, 1, 1001)\nf1s = [f1_score(y_test, (probs >= t).astype(int), zero_division=0) for t in ths]\nbest_thr = float(ths[int(np.argmax(f1s))])\ny_pred_best = (probs >= best_thr).astype(int)\n\nmetrics.update({\n    \"best_threshold\": best_thr,\n    \"accuracy@best\": float(accuracy_score(y_test, y_pred_best)),\n    \"precision@best\": float(precision_score(y_test, y_pred_best, zero_division=0)),\n    \"recall@best\": float(recall_score(y_test, y_pred_best, zero_division=0)),\n    \"f1@best\": float(f1_score(y_test, y_pred_best, zero_division=0)),\n})\n\nwith open(OUT / \"metrics_ensemble.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2)\n\n# ============================================================\n# Plots: ROC, PR, Confusion Matrices\n# ============================================================\n# ROC\nfpr, tpr, _ = roc_curve(y_test, probs)\nplt.figure()\nplt.plot(fpr, tpr, label=f\"Ensemble AUC={metrics['roc_auc']:.3f}\")\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.legend()\nplt.title(\"ROC Curve (Ensemble)\")\nplt.savefig(OUT / \"ensemble_roc.png\")\nplt.close()\n\n# PR\nprec, rec, _ = precision_recall_curve(y_test, probs)\nplt.figure()\nplt.plot(rec, prec, label=f\"AP={metrics['avg_precision']:.3f}\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.legend()\nplt.title(\"Precision-Recall Curve (Ensemble)\")\nplt.savefig(OUT / \"ensemble_pr.png\")\nplt.close()\n\n# Confusion matrices\ndef plot_cm(y_true, y_pred, title, fname):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure()\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n    plt.title(title)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    plt.savefig(OUT / fname)\n    plt.close()\n\nplot_cm(y_test, y_pred_05, \"Confusion Matrix (thr=0.5)\", \"cm_thr_0.5.png\")\nplot_cm(y_test, y_pred_best, f\"Confusion Matrix (thr={best_thr:.3f})\", \"cm_thr_best.png\")\njson.dump({\"best_threshold\": best_thr}, open(OUT / \"best_threshold.json\", \"w\"))\n\nprint(\"Artifacts saved to:\", OUT)\nprint(sorted(os.listdir(OUT)))\n\n# ============================================================\n# SHAP Explainability (values, comparisons)\n# ============================================================\ndef _shap_values_binary(explainer, X_):\n    \"\"\"Return SHAP values for positive class consistently across SHAP versions.\"\"\"\n    sv = explainer.shap_values(X_)\n    if isinstance(sv, list):  # [class0, class1]\n        return sv[1]\n    return sv\n\ndef _base_value_binary(explainer):\n    \"\"\"Return scalar base value for positive class robustly.\"\"\"\n    bv = explainer.expected_value\n    # SHAP may return list/array for [neg, pos]; pick pos if length 2\n    if isinstance(bv, (list, np.ndarray)) and len(np.atleast_1d(bv)) >= 2:\n        return float(np.atleast_1d(bv)[1])\n    # else assume scalar\n    return float(bv)\n\n# Use first model for classical SHAP plots\nexplainer0 = shap.TreeExplainer(models[0])\nsv0 = _shap_values_binary(explainer0, X_test)\nbase_val = _base_value_binary(explainer0)\n\n# SHAP summary (beeswarm)\nplt.figure()\nshap.summary_plot(sv0, X_test, plot_type=\"dot\", show=False)\nplt.savefig(OUT / \"shap_summary_beeswarm.png\", bbox_inches=\"tight\")\nplt.close()\n\n# SHAP bar importance (top 20)\nplt.figure()\nshap.summary_plot(sv0, X_test, plot_type=\"bar\", max_display=20, show=False)\nplt.savefig(OUT / \"shap_summary_bar.png\", bbox_inches=\"tight\")\nplt.close()\n\n# Single-sample force plot (robust to SHAP>=0.40)\nshap.initjs()\nsample_idx = int(np.where(y_test.values == 1)[0][0]) if y_test.sum() > 0 else 0\n# Static PNG via matplotlib mode\nplt.figure()\nshap.force_plot(base_val, sv0[sample_idx, :], X_test.iloc[sample_idx, :], matplotlib=True, show=False)\nplt.savefig(OUT / \"shap_force_sample.png\", bbox_inches=\"tight\")\nplt.close()\n\n# Compare importances across ensemble\ndef mean_abs_shap(sv):\n    return np.abs(sv).mean(axis=0)\n\nsv_list = []\nfor m in models:\n    e = shap.TreeExplainer(m)\n    sv = _shap_values_binary(e, X_test)\n    sv_list.append(sv)\n\nsv_mean = np.mean(sv_list, axis=0)  # (n_samples, n_features)\n\ndf_imp = pd.DataFrame(\n    {f\"Model{i+1}\": mean_abs_shap(sv_list[i]) for i in range(len(sv_list))},\n    index=X_test.columns,\n)\ndf_imp[\"Ensemble\"] = mean_abs_shap(sv_mean)\n\ndf_top = df_imp / df_imp.max()\ntop20 = df_top.sort_values(\"Ensemble\", ascending=False).head(20)\nplt.figure(figsize=(12, 6))\ntop20.plot(kind=\"bar\")\nplt.title(\"Top-20 Feature Importance Comparison (SHAP mean |value|)\")\nplt.tight_layout()\nplt.savefig(OUT / \"shap_comparison_top20.png\")\nplt.close()\n\n# Correlation of importances across models\nplt.figure(figsize=(6, 5))\nsns.heatmap(df_imp.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\nplt.title(\"Correlation of SHAP Importances Across Models\")\nplt.tight_layout()\nplt.savefig(OUT / \"shap_importance_correlation.png\")\nplt.close()\n\n# ============================================================\n# SHAP INTERACTION VALUES (subsample for speed)\n# ============================================================\n# NOTE: SHAP interaction values can be heavy. We'll subsample to up to 2000 rows.\nmax_inter_rows = 2000\nX_inter = X_test.sample(n=max_inter_rows, random_state=42) if len(X_test) > max_inter_rows else X_test.copy()\n\nexplainer_int = shap.TreeExplainer(models[0])\nsv_int = explainer_int.shap_interaction_values(X_inter)\nif isinstance(sv_int, list):\n    sv_int = sv_int[1]  # (n, f, f)\n\n# Mean absolute interaction strength matrix\nmean_abs_int = np.abs(sv_int).mean(axis=0)  # (f, f)\nfeat_names = X_inter.columns\n\n# Heatmap of interaction strengths\nplt.figure(figsize=(8, 7))\nsns.heatmap(mean_abs_int, xticklabels=feat_names, yticklabels=feat_names, cmap=\"viridis\")\nplt.title(\"SHAP Interaction Values (mean |interaction|)\")\nplt.tight_layout()\nplt.savefig(OUT / \"shap_interaction_heatmap.png\")\nplt.close()\n\n# Extract top pairwise interactions (upper triangle)\ntri_idx = np.triu_indices_from(mean_abs_int, k=1)\npairs = [((feat_names[i], feat_names[j]), mean_abs_int[i, j]) for i, j in zip(*tri_idx)]\npairs_sorted = sorted(pairs, key=lambda x: x[1], reverse=True)\ntop_pairs = pairs_sorted[:3]  # top-3 interactions\n\n# Dependence plots for top-3 interactions\nfor k, ((f_main, f_int), strength) in enumerate(top_pairs, start=1):\n    plt.figure()\n    shap.dependence_plot(\n        f_main,\n        sv0 if sv0.shape[0] == X_test.shape[0] else _shap_values_binary(explainer0, X_test),\n        X_test,\n        interaction_index=f_int,\n        show=False,\n    )\n    plt.title(f\"SHAP Dependence: {f_main} (interaction w/ {f_int})\")\n    plt.tight_layout()\n    plt.savefig(OUT / f\"shap_dependence_{k}_{f_main}_x_{f_int}.png\")\n    plt.close()\n\n# Also save a small JSON summary of top interactions\ntop_inter_json = [{\"main\": a, \"interaction\": b, \"mean_abs_strength\": float(s)} for (a, b), s in top_pairs]\njson.dump(top_inter_json, open(OUT / \"shap_top_interactions.json\", \"w\"), indent=2)\n\n# ============================================================\n# Final prints\n# ============================================================\nprint(\"\\n=== FINAL METRICS ===\")\nprint(json.dumps(metrics, indent=2))\nprint(\"\\nArtifacts saved to:\", OUT)\nprint(sorted(os.listdir(OUT)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T17:46:43.313217Z","iopub.execute_input":"2025-08-16T17:46:43.313868Z","iopub.status.idle":"2025-08-16T18:01:31.610766Z","shell.execute_reply.started":"2025-08-16T17:46:43.313844Z","shell.execute_reply":"2025-08-16T18:01:31.610196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Save X_test and y_test for Gradio Dashboard\n# ============================================================\nX_test.to_parquet(PROC/\"X_test.parquet\", index=False)\njoblib.dump(y_test, PROC/\"y_test.pkl\")\n\nprint(\"Saved X_test and y_test to:\", PROC)\nprint(os.listdir(PROC))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:07:55.911086Z","iopub.execute_input":"2025-08-16T18:07:55.911705Z","iopub.status.idle":"2025-08-16T18:07:56.199263Z","shell.execute_reply.started":"2025-08-16T18:07:55.911685Z","shell.execute_reply":"2025-08-16T18:07:56.198418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Fraud Detection Gradio Dashboard (fixed image return: numpy)\n# ============================================================\nimport gradio as gr\nimport shap, json, joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np, pandas as pd\nfrom io import BytesIO\nfrom pathlib import Path\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom PIL import Image\n\n# === Load Artifacts ===\nOUT = Path(\"/kaggle/working/out/fraud_model\")\nPROC = Path(\"/kaggle/working/data/fraud/processed\")\n\nfeat_order_loaded = json.load(open(PROC/\"feature_order.json\"))\nscaler_loaded = joblib.load(PROC/\"scaler_normAmount.pkl\")\nmodels = [joblib.load(OUT/f) for f in [\"lgb_fraud_ens_seed_11.pkl\",\n                                       \"lgb_fraud_ens_seed_22.pkl\",\n                                       \"lgb_fraud_ens_seed_33.pkl\"]]\nbest_thr = json.load(open(OUT/\"best_threshold.json\"))[\"best_threshold\"]\n\nX_test = pd.read_parquet(PROC/\"X_test.parquet\")\ny_test = joblib.load(PROC/\"y_test.pkl\")\n\n# === Helpers ===\ndef fig_to_numpy():\n    \"\"\"Convert current matplotlib figure to numpy array (RGB).\"\"\"\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n    buf.seek(0)\n    img = Image.open(buf).convert(\"RGB\")\n    arr = np.array(img)\n    plt.close()\n    return arr\n\n# === Prediction Helper ===\ndef predict_sample(sample_dict, scaler, feature_order, models, thr):\n    df_in = pd.DataFrame([sample_dict])\n    if \"Time\" in df_in: df_in = df_in.drop(\"Time\", axis=1)\n    if \"Amount\" in df_in:\n        df_in[\"normAmount\"] = scaler.transform(df_in[[\"Amount\"]])\n        df_in = df_in.drop(\"Amount\", axis=1)\n    for col in feature_order:\n        if col not in df_in: df_in[col] = 0\n    df_in = df_in[feature_order]\n    # Ensemble prob\n    probs = np.mean([m.predict(df_in, num_iteration=m.best_iteration) for m in models], axis=0)\n    prob = float(probs[0])\n    label = int(prob >= thr)\n    return df_in, {\"probability\": prob, \"label\": label}\n\n# === SHAP explainer ===\nexplainer = shap.TreeExplainer(models[0])\n\ndef shap_plot(df_in):\n    shap_vals = explainer.shap_values(df_in)\n    if isinstance(shap_vals, list):  # binary -> take class 1\n        shap_vals = shap_vals[1]\n    plt.figure(figsize=(8,4))\n    shap.summary_plot(shap_vals, df_in, plot_type=\"bar\", show=False)\n    return fig_to_numpy()\n\ndef shap_summary(X):\n    shap_vals = explainer.shap_values(X[:200])\n    if isinstance(shap_vals, list):\n        shap_vals = shap_vals[1]\n    plt.figure(figsize=(8,5))\n    shap.summary_plot(shap_vals, X[:200], show=False)\n    return fig_to_numpy()\n\n# === Gradio predict functions ===\nexample_input = json.dumps(X_test.iloc[0].to_dict(), indent=2)\n\ndef gradio_predict(json_str):\n    try:\n        sample = json.loads(json_str)\n    except:\n        return {\"error\": \"Invalid JSON input\"}, None\n    df_in, out = predict_sample(sample, scaler_loaded, feat_order_loaded, models, best_thr)\n    shap_img = shap_plot(df_in)\n    return out, shap_img\n\ndef gradio_batch(file_obj):\n    df = pd.read_csv(file_obj.name)\n    preds = []\n    for i, row in df.iterrows():\n        df_in, out = predict_sample(row.to_dict(), scaler_loaded, feat_order_loaded, models, best_thr)\n        preds.append(out)\n    pred_df = pd.DataFrame(preds)\n    tmp_path = \"/kaggle/working/batch_predictions.csv\"\n    pred_df.to_csv(tmp_path, index=False)\n    global_shap_img = shap_summary(X_test)\n    return pred_df, tmp_path, global_shap_img\n\n# === Metrics Dashboard ===\ndef get_metrics_and_cm():\n    probs = np.mean([m.predict(X_test, num_iteration=m.best_iteration) for m in models], axis=0)\n    y_pred = (probs >= best_thr).astype(int)\n    report = classification_report(y_test, y_pred, output_dict=True)\n    auc = roc_auc_score(y_test, probs)\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure()\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n    cm_img = fig_to_numpy()\n\n    return {\n        \"Accuracy\": report[\"accuracy\"],\n        \"Precision\": report[\"1\"][\"precision\"],\n        \"Recall\": report[\"1\"][\"recall\"],\n        \"F1\": report[\"1\"][\"f1-score\"],\n        \"AUC\": auc\n    }, cm_img\n\n# === Gradio UI ===\nwith gr.Blocks() as demo:\n    gr.Markdown(\"## Fraud Detection Dashboard 🚀 (LightGBM Ensemble + SHAP + Metrics)\")\n    \n    with gr.Tab(\"🔍 Single Transaction\"):\n        inp = gr.Textbox(label=\"Transaction JSON\", lines=12, value=example_input)\n        out_json = gr.JSON(label=\"Prediction\")\n        out_img = gr.Image(type=\"numpy\", label=\"SHAP Feature Importance\")\n        btn = gr.Button(\"Predict\")\n        btn.click(fn=gradio_predict, inputs=inp, outputs=[out_json, out_img])\n    \n    with gr.Tab(\"📂 Batch Prediction\"):\n        file_in = gr.File(label=\"Upload CSV\", file_types=[\".csv\"])\n        out_tbl = gr.Dataframe(label=\"Batch Predictions\")\n        out_file = gr.File(label=\"Download Predictions (CSV)\")\n        out_global = gr.Image(type=\"numpy\", label=\"Global SHAP Summary (Beeswarm)\")\n        file_in.upload(fn=gradio_batch, inputs=file_in, outputs=[out_tbl, out_file, out_global])\n\n    with gr.Tab(\"📊 Metrics Dashboard\"):\n        btn2 = gr.Button(\"Compute Test Metrics + Confusion Matrix\")\n        out_metrics = gr.JSON(label=\"Metrics\")\n        out_cm = gr.Image(type=\"numpy\", label=\"Confusion Matrix\")\n        btn2.click(fn=get_metrics_and_cm, inputs=None, outputs=[out_metrics, out_cm])\n\n# Launch\ndemo.launch(share=True, debug=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:27:55.908072Z","iopub.execute_input":"2025-08-16T18:27:55.908356Z","iopub.status.idle":"2025-08-16T18:30:10.040603Z","shell.execute_reply.started":"2025-08-16T18:27:55.908334Z","shell.execute_reply":"2025-08-16T18:30:10.039864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Save Final Fraud Detection Model (Ensemble + Metadata) - No Calibration\n# ============================================================\nimport joblib, json\nfrom pathlib import Path\n\nOUT = Path(\"/kaggle/working/out/fraud_model\")\nPROC = Path(\"/kaggle/working/data/fraud/processed\")\n\nfinal_model_bundle = {\n    \"models\": [\n        joblib.load(OUT/\"lgb_fraud_ens_seed_11.pkl\"),\n        joblib.load(OUT/\"lgb_fraud_ens_seed_22.pkl\"),\n        joblib.load(OUT/\"lgb_fraud_ens_seed_33.pkl\")\n    ],\n    \"calibrator\": None,  # no calibration\n    \"scaler\": joblib.load(PROC/\"scaler_normAmount.pkl\"),\n    \"feature_order\": json.load(open(PROC/\"feature_order.json\")),\n    \"best_threshold\": json.load(open(OUT/\"best_threshold.json\"))[\"best_threshold\"],\n    \"metrics\": json.load(open(OUT/\"metrics_ensemble.json\"))\n}\n\njoblib.dump(final_model_bundle, OUT/\"final_fraud_model.pkl\")\nprint(\"✅ Final model bundle saved (no calibration):\", OUT/\"final_fraud_model.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:31:29.145986Z","iopub.execute_input":"2025-08-16T18:31:29.146223Z","iopub.status.idle":"2025-08-16T18:31:29.240888Z","shell.execute_reply.started":"2025-08-16T18:31:29.146207Z","shell.execute_reply":"2025-08-16T18:31:29.240095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Get ensemble raw probs\nprobs = np.mean([m.predict(X_test, num_iteration=m.best_iteration) for m in models], axis=0).reshape(-1,1)\n\n# Train calibration model\ncalib = LogisticRegression(max_iter=1000)\ncalib.fit(probs, y_test)\n\n# Save calibration model\njoblib.dump(calib, OUT/\"calibration_lr.pkl\")\nprint(\"✅ Calibration model saved\")\n\n# Then re-run the final bundle saving code\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:31:18.525293Z","iopub.execute_input":"2025-08-16T18:31:18.525591Z","iopub.status.idle":"2025-08-16T18:31:20.926800Z","shell.execute_reply.started":"2025-08-16T18:31:18.525560Z","shell.execute_reply":"2025-08-16T18:31:20.925290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bundle = joblib.load(\"/kaggle/working/out/fraud_model/final_fraud_model.pkl\")\n\nmodels = bundle[\"models\"]\ncalib = bundle[\"calibrator\"]\nscaler = bundle[\"scaler\"]\nfeat_order = bundle[\"feature_order\"]\nthr = bundle[\"best_threshold\"]\n\nprint(\"Loaded final model with best threshold:\", thr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:31:44.028085Z","iopub.execute_input":"2025-08-16T18:31:44.028649Z","iopub.status.idle":"2025-08-16T18:31:44.052500Z","shell.execute_reply.started":"2025-08-16T18:31:44.028628Z","shell.execute_reply":"2025-08-16T18:31:44.051722Z"}},"outputs":[],"execution_count":null}]}